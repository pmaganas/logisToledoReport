#!/usr/bin/env python3
"""
Script de prueba para verificar las optimizaciones de rendimiento del generador de informes.
Este script simula la generaciÃ³n de un informe y mide los tiempos de ejecuciÃ³n.
"""

import os
import sys
import time
import logging
from datetime import datetime, timedelta

# Add the project root to Python path
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

def test_report_generation():
    """Prueba la generaciÃ³n de informes con las optimizaciones implementadas"""
    
    print("ğŸš€ INICIANDO PRUEBA DE OPTIMIZACIONES DE RENDIMIENTO")
    print("=" * 60)
    
    try:
        # Import here to avoid issues if the app isn't properly set up
        from services.no_breaks_report_generator import NoBreaksReportGenerator
        from services.check_types_service import CheckTypesService
        
        print("âœ… MÃ³dulos importados correctamente")
        
        # Initialize services
        print("ğŸ”„ Inicializando servicios...")
        start_time = time.time()
        
        generator = NoBreaksReportGenerator()
        check_types_service = CheckTypesService()
        
        init_time = time.time() - start_time
        print(f"âœ… Servicios inicializados en {init_time:.2f}s")
        
        # Test check types caching
        print("ğŸ”„ Probando cache de check types...")
        cache_start = time.time()
        
        success = check_types_service.ensure_check_types_cached()
        
        cache_time = time.time() - cache_start
        print(f"âœ… Cache de check types: {'OK' if success else 'FALLO'} en {cache_time:.2f}s")
        
        # Test report generation (this will depend on having valid API tokens)
        print("ğŸ”„ Probando generaciÃ³n de informe de prueba...")
        
        # Calculate date range (last 7 days)
        end_date = datetime.now()
        start_date = end_date - timedelta(days=7)
        
        from_date = start_date.strftime('%Y-%m-%d')
        to_date = end_date.strftime('%Y-%m-%d')
        
        print(f"ğŸ“… Rango de fechas: {from_date} hasta {to_date}")
        
        # Progress callback for monitoring
        def progress_callback(page, total_pages, current_records, total_records):
            print(f"ğŸ“Š Progreso API: PÃ¡gina {page}/{total_pages} - Registros: {current_records}/{total_records}")
        
        # Generate report
        report_start = time.time()
        
        report_data = generator.generate_report(
            from_date=from_date,
            to_date=to_date,
            report_type="by_employee",
            format="xlsx",
            progress_callback=progress_callback
        )
        
        report_time = time.time() - report_start
        
        if report_data:
            report_size = len(report_data) if report_data else 0
            print(f"âœ… Informe generado en {report_time:.2f}s - TamaÃ±o: {report_size:,} bytes")
            
            # Performance analysis
            print("\nğŸ“ˆ ANÃLISIS DE RENDIMIENTO:")
            print(f"â±ï¸  Tiempo de inicializaciÃ³n: {init_time:.2f}s")
            print(f"ğŸ’¾ Tiempo de cache: {cache_time:.2f}s") 
            print(f"ğŸ“Š Tiempo de generaciÃ³n: {report_time:.2f}s")
            print(f"ğŸ¯ Tiempo total: {(init_time + cache_time + report_time):.2f}s")
            
            # Performance benchmarks
            if report_time < 60:
                print("ğŸŸ¢ EXCELENTE: GeneraciÃ³n en menos de 1 minuto")
            elif report_time < 300:
                print("ğŸŸ¡ BUENO: GeneraciÃ³n en menos de 5 minutos")
            elif report_time < 900:
                print("ğŸŸ  ACEPTABLE: GeneraciÃ³n en menos de 15 minutos")
            else:
                print("ğŸ”´ LENTO: GeneraciÃ³n toma mÃ¡s de 15 minutos")
                
        else:
            print(f"âš ï¸  Informe vacÃ­o generado en {report_time:.2f}s")
            print("â„¹ï¸  Esto puede ser normal si no hay datos en el rango de fechas especificado")
        
        print("\nğŸ‰ PRUEBA COMPLETADA EXITOSAMENTE")
        
    except ImportError as e:
        print(f"âŒ Error de importaciÃ³n: {e}")
        print("â„¹ï¸  AsegÃºrate de que la base de datos estÃ© configurada y las dependencias instaladas")
        return False
        
    except Exception as e:
        print(f"âŒ Error durante la prueba: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    return True

def test_api_performance():
    """Prueba especÃ­ficamente el rendimiento de la API paralela"""
    
    print("\nğŸ”— PROBANDO RENDIMIENTO DE API PARALELA")
    print("=" * 50)
    
    try:
        from services.parallel_sesame_api import ParallelSesameAPI
        from services.sesame_api import SesameAPI
        
        # Test connection
        parallel_api = ParallelSesameAPI()
        regular_api = SesameAPI()
        
        print("ğŸ”„ Probando conectividad...")
        
        # Test token info
        token_start = time.time()
        token_info = parallel_api.get_token_info()
        token_time = time.time() - token_start
        
        if token_info:
            print(f"âœ… Token vÃ¡lido - Respuesta en {token_time:.2f}s")
            print(f"â„¹ï¸  Empresa: {token_info.get('company', {}).get('name', 'N/A')}")
        else:
            print("âŒ Token invÃ¡lido o sin conexiÃ³n")
            return False
        
        # Test small data fetch
        print("ğŸ”„ Probando fetch de datos pequeÃ±o...")
        
        end_date = datetime.now()
        start_date = end_date - timedelta(days=1)  # Just 1 day for quick test
        
        small_fetch_start = time.time()
        
        small_data = parallel_api.get_time_tracking(
            from_date=start_date.strftime('%Y-%m-%d'),
            to_date=end_date.strftime('%Y-%m-%d'),
            page=1,
            limit=100
        )
        
        small_fetch_time = time.time() - small_fetch_start
        
        if small_data and small_data.get('data'):
            records_count = len(small_data['data'])
            print(f"âœ… Fetch pequeÃ±o completado en {small_fetch_time:.2f}s - {records_count} registros")
        else:
            print(f"âš ï¸  Fetch pequeÃ±o sin datos en {small_fetch_time:.2f}s")
        
        print("\nğŸ“Š MÃ‰TRICAS DE RENDIMIENTO API:")
        print(f"ğŸ”— Conectividad: {token_time:.2f}s")
        print(f"ğŸ“¥ Fetch datos: {small_fetch_time:.2f}s")
        
        return True
        
    except Exception as e:
        print(f"âŒ Error en prueba de API: {e}")
        import traceback
        traceback.print_exc()
        return False

def main():
    """FunciÃ³n principal de prueba"""
    
    print("ğŸ§ª SUITE DE PRUEBAS DE OPTIMIZACIONES DE RENDIMIENTO")
    print("=" * 70)
    print(f"ğŸ“… Fecha: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print()
    
    # Test API performance first
    api_success = test_api_performance()
    
    if api_success:
        # Test report generation if API works
        report_success = test_report_generation()
        
        if report_success:
            print("\nğŸ¯ RESUMEN FINAL:")
            print("âœ… Todas las optimizaciones funcionan correctamente")
            print("ğŸš€ El sistema deberÃ­a generar informes significativamente mÃ¡s rÃ¡pido")
            print("\nğŸ“‹ OPTIMIZACIONES IMPLEMENTADAS:")
            print("  â€¢ âš¡ Procesamiento paralelo de API (8 workers)")
            print("  â€¢ ğŸ“Š PaginaciÃ³n optimizada (1000 registros/pÃ¡gina)")
            print("  â€¢ ğŸ’¾ Cache de empleados y actividades")
            print("  â€¢ ğŸ“ Escritura Excel por lotes")
            print("  â€¢ ğŸ”— Conexiones HTTP optimizadas")
            
        else:
            print("\nâš ï¸  Las pruebas de generaciÃ³n de informes fallaron")
            
    else:
        print("\nâŒ Las pruebas de API fallaron - verifica la configuraciÃ³n de tokens")
    
    print("\n" + "=" * 70)
    print("ğŸ FIN DE LAS PRUEBAS")

if __name__ == "__main__":
    main()